f <- function(p){
abs(qnorm(1-p/2)-sqrt(2)*qnorm(1-(0.05-p)/2))
}
nlm(f,0.025)
f <- function(p){
abs(qnorm(1-p/2)-sqrt(2)*qnorm(1-(0.05-p)/2))
}
nlm(f,0.03)
f <- function(p){
abs(qnorm(1-p/2)-sqrt(2)*qnorm(1-(0.05-p)/2))
}
nlm(f,0.04)
nlm(f,0.04)$estimate
qnorm(1-nlm(f,0.04)$estimate/2)
qnorm(0.9875)
sqrt(2)*qnorm)(0.9875)
sqrt(2)*qnorm(0.9875)
qnorm(1-nlm(f,0.03)$estimate/2)
qnorm(1-nlm(f,0.02)$estimate/2)
x <- c(-0.289,  0.054, -1.946, -1.810, -1.611, 1.114,  1.997,  1.602, -1.063, -0.443, -1.379, -1.324, -1.274, -0.741,  0.962, 0.4866,  0.530,  0.575,  0.285, -1.622, 1.072, 0.086, -1.382, 0.213, 1.208, 0.358, 1.946, -1.115, -1.308, -0.613, -1.253, 0.615, -1.432,  0.361, 0.319, -0.269, -0.756, 1.487,  0.421, -0.599)
y <-c(1.515, -0.514, -3.367, -1.216,  1.653, -3.682, 4.734, -1.093,  4.436, 2.459, 2.679,  3.282, 4.238,  2.984, -3.330, -2.251, -3.256, -3.454, -1.474, 1.461, -4.461, -1.603,  3.069, -2.293, -3.747, -1.929, 2.522,  3.071,  2.908,  3.788, 4.154, -3.195,  2.822, -1.969, -2.304, 1.906, 3.11, -2.620, -2.861,  3.077)
n <- length(x)
plot(x,y)
lm0 <- lm(y~x)
abline(lm0,col="blue")
x_mat <- rep(1,40)
for (i in 1:3){
x_mat<-cbind(x_mat,x^i)
}
beta_hat <- solve((t(x_mat) %*% x_mat)) %*% t(x_mat) %*% y
plot <-function(p){
fun<-function(x){
y<-0
for (i in 1:p){
y <- y+beta_hat[i]*x^(i-1)
}
return(y)
}
curve(fun,from=-2,to=2,add=T,col=p-1)
}
plot(4)
x_mat <- rep(1,40)
for (i in 1:14){
x_mat<-cbind(x_mat,x^i)
}
beta_hat <- solve((t(x_mat) %*% x_mat)) %*% t(x_mat) %*% y
plot(15)
x_mat <- rep(1,40)
for (i in 1:19){
x_mat<-cbind(x_mat,x^i)
}
colnames(x_mat) <- 1:20
data_cv <- data.frame(y,x_mat)
cv1 <- c()
for (i in 1:20){
r <- c()
for (j in 1:40){
data1 <- data_cv[-j,1:(i+1)]
lm0 <- lm(y~.,data=data1)
r[j] <- (data_cv[j,1]-predict(lm0,data_cv[j,1:(i+1)]))^2
}
cv1[i] <- mean(r,na.rm=T)
}
cv1
plot(1:20,cv1)
plot(1:20,cv1[1:20])
x_mat <- rep(1,40)
for (i in 1:19){
x_mat<-cbind(x_mat,x^i)
}
colnames(x_mat) <- 1:20
data_cv <- data.frame(y,x_mat)
cv1 <- c()
for (i in 1:20){
r <- c()
for (j in 1:40){
data1 <- data_cv[-j,1:(i+1)]
lm0 <- lm(y~.,data=data1)
r[j] <- (data_cv[j,1]-predict(lm0,data_cv[j,1:(i+1)]))^2
}
cv1[i] ＝ mean(r,na.rm=T)
}
cv1
plot(1:20,cv1[1:20])
x_mat <- rep(1,40)
for (i in 1:19){
x_mat<-cbind(x_mat,x^i)
}
colnames(x_mat) <- 1:20
data_cv <- data.frame(y,x_mat)
cv1 <- c()
for (i in 1:20){
r <- c()
for (j in 1:40){
data1 <- data_cv[-j,1:(i+1)]
lm0 <- lm(y~.,data=data1)
r[j] <- (data_cv[j,1]-predict(lm0,data_cv[j,1:(i+1)]))^2
}
cv1[i] <- mean(r,na.rm=T)
}
cv1
plot(1:20,cv1)
cv1[2]
plot(x=1:20,y=cv1)
plot(c(1:20),cv1)
mode(cv1)
typeof(cv1)
plot(1:20,1:20)
plot(1:20,1:20)
plot(1:20,1:20)
x <- c(-0.289,  0.054, -1.946, -1.810, -1.611, 1.114,  1.997,  1.602, -1.063, -0.443, -1.379, -1.324, -1.274, -0.741,  0.962, 0.4866,  0.530,  0.575,  0.285, -1.622, 1.072, 0.086, -1.382, 0.213, 1.208, 0.358, 1.946, -1.115, -1.308, -0.613, -1.253, 0.615, -1.432,  0.361, 0.319, -0.269, -0.756, 1.487,  0.421, -0.599)
y <-c(1.515, -0.514, -3.367, -1.216,  1.653, -3.682, 4.734, -1.093,  4.436, 2.459, 2.679,  3.282, 4.238,  2.984, -3.330, -2.251, -3.256, -3.454, -1.474, 1.461, -4.461, -1.603,  3.069, -2.293, -3.747, -1.929, 2.522,  3.071,  2.908,  3.788, 4.154, -3.195,  2.822, -1.969, -2.304, 1.906, 3.11, -2.620, -2.861,  3.077)
x_mat <- rep(1,40)
for (i in 1:19){
x_mat<-cbind(x_mat,x^i)
}
colnames(x_mat) <- 1:20
data_cv <- data.frame(y,x_mat)
cv1 <- c()
for (i in 1:20){
r <- c()
for (j in 1:40){
data1 <- data_cv[-j,1:(i+1)]
lm0 <- lm(y~.,data=data1)
r[j] <- (data_cv[j,1]-predict(lm0,data_cv[j,1:(i+1)]))^2
}
cv1[i] <- mean(r,na.rm=T)
}
cv1
plot(c(1:20),cv1)
plot(1:20,1:20)
x_mat <- rep(1,40)
for (i in 1:19){
x_mat<-cbind(x_mat,x^i)
}
colnames(x_mat) <- 1:20
data_cv <- data.frame(y,x_mat)
cv1 <- c()
for (i in 1:20){
r <- c()
for (j in 1:40){
data1 <- data_cv[-j,1:(i+1)]
lm0 <- lm(y~.,data=data1)
r[j] <- (data_cv[j,1]-predict(lm0,data_cv[j,1:(i+1)]))^2
}
cv1[i] <- mean(r,na.rm=T)
}
cv1
plot(c(1:20),cv1)
x_mat <- rep(1,40)
for (i in 1:19){
x_mat<-cbind(x_mat,x^i)
}
solve((t(x_mat) %*% x_mat))
cv1
i=20
data1 <- data_cv[-j,1:(i+1)]
j=1
data1 <- data_cv[-j,1:(i+1)]
data1
lm0 <- lm(y~.,data=data1)
lm0
r[j] <- (data_cv[j,1]-predict(lm0,data_cv[j,1:(i+1)]))^2
r[1]
for (j in 1:40){
data1 <- data_cv[-j,1:(i+1)]
lm0 <- lm(y~.,data=data1)
r[j] <- (data_cv[j,1]-predict(lm0,data_cv[j,1:(i+1)]))^2
}
r
j=3
data1 <- data_cv[-j,1:(i+1)]
data1
lm0 <- lm(y~.,data=data1)
lm0
data_cv[j,1]
predict(lm0,data_cv[j,1:(i+1)])
data_cv[j,1:(i+1)]
qt(4/3)
qt(4/3,16)
pt(4/3,16)
a<-pt(4/3,16)
2*(1-a)
m=8
n=6
a<-1:5
sd(a)
sd(a)^2
x <- c(1.23,1.42,1.41,1.62,1.55,1.51,1.6,1.76)
y <- c(1.76,1.41,1.87,1.49,1.67,1.81)
mean(x)
mean(y)
sm<-sd(x)^2*7
sn <- sd(y)^2*5
u <- sqrt(m+n-2)*(mean(x)-mean(y))/sqrt(1/m+1/n)/sqrt(sm+sn)
u
m+n-2
qt(0.9,12)
qf(0.95.15.9)
qf(0.95,15,9)
122/45/(39.6/9)
??chisquare
pchisq(27.51,4)
1-pchisq(27.51,4)
1-pchisq(3.609,3)
1-pchisq(11.21,2)
1-pchisq(11.21,4)
1-pchisq(1.211,1)
1-pchisq(1.211,3)
1-pchisq(0.91,1)
op <- par()
op
par(mfrow(c(2,2)))
par(mfrow=c(2,2))
plot(1:10,1:10)
par(op)
plot(1:10,1:10)
a <- matrix(c(1,2,2,4),col=2)
?matrix
a <- matrix(c(1,2,2,4),ncol=2)
a <- matrix(c(1,2,2,4),ncol=2)
a
b <- matrix(c(1,3,2,4),ncol=2)
b
a %*% b
a+b
rank(a)
r(a)
eigen(a)
det(a)
det(b)
x<-c(2,1)
x%*%a%*%x
x%*%t(x)
t(x)%*%x
sqrt(8^2+15^2)
1/3*(-4)^3-1/2*16+24+27/2
1/3*4^3-1/2*16-24+27/2
1/3*(-2)^3-1/2*(-2)^2+12+27/2
8.166667*3
8.16666666667*6
49/6
2.8333333*3
2.833333*6
20.8333333*6
-0.25*log(0.25)-0.75*log(0.75)
-0.5*log(0.5)*2
1/3*3^3-1/2*3^2-6*3+27/2
packages.used=c("rvest", "tibble", "qdap",
"sentimentr", "gplots", "dplyr",
"tm", "syuzhet", "factoextra",
"beeswarm", "scales", "RColorBrewer",
"RANN", "tm", "topicmodels")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
print(R.version)
knit_with_parameters('~/Documents/Michelle/Columbia stat/ads/Tutorial2/doc/wk2-Tutorial-TextMining.Rmd')
library("rmarkdown", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
install.packages("rmarkdown")
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane,
#    w[d+1] = -c is the negative offset parameter.
#n: sample size
#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels
fakedata <- function(w, n){
if(! require(MASS))
{
install.packages("MASS")
}
if(! require(mvtnorm))
{
install.packages("mvtnorm")
}
require(MASS)
require(mvtnorm)
# obtain dimension
d <- length(w)-1
# compute the offset vector and a Basis consisting of w and its nullspace
offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
Basis <- cbind(Null(w[1:d]), w[1:d])
# Create samples, correct for offset, and extend
# rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis)
S <- S + matrix(rep(offset,n),n,d,byrow=T)
S <- cbind(S,1)
# compute the class assignments
y <- as.vector(sign(S %*% w))
# add corrective factors to points that lie on the hyperplane.
S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
y = as.vector(sign(S %*% w))
return(list(S=S, y=y))
} # end function fakedata
#Statistical Machine Learning (GR5241) HW2
#name: Chengcheng Yuan uni: cy2434
# Question 3. Perceptron
# 1.
classify <- function(S,z){
yhat <- as.vector(sign(S %*% z))
return(yhat)
}
#2.
perceptrain <- function(S,y){
n <- ncol(S)
z0 <- rep(1/2,n)
max.iter <- 200
step.size <- 1/max.iter
zmat <- matrix(0, nrow = max.iter, ncol = n)
zmat[1,] <- z0
for (k in 2:max.iter){
index <- as.numeric(classify(S,zmat[k-1, ])!=y)
cp <- sum(index*abs(classify(S,zmat[k-1, ])))
if (cp == 0) {
k <- k-1; break
}
a <- index*(-y)
cpgrad <- colSums(a*S)
zmat[k, ] <- zmat[k-1, ] - step.size * cpgrad
}
zmat <- zmat[1:k, ] # Trim
return(list(z = zmat[k, ], Z_history = zmat))
}
# check the perceptron function
fake <- fakedata(c(1,2,-3),100)
perceptron <- perceptrain(fake$S,fake$y)
perceptron$z
yhat <- classify(fake$S,perceptron$z)
sum(yhat==fake$y) #number of data that is classified correctly from the 100 data sample
#We can see from the result that the training data has all been classified correctly using my Perceptron training algorithm.
#3.
z_original <- sample(1:10,3) #generate a new three-dimensional random vector z
z_original
training <- fakedata(z_original,100)
perceptron <- perceptrain(training$S,training$y)
perceptron$z
test <- fakedata(z_original,100)
yhat <- classify(test$S,perceptron$z)
sum(yhat==test$y)/100
# The precision rate is usually above 95%, my classifier performs pretty well.
#4.
#First plot
plot(test$S[,1],test$S[,2],xlab="x1",ylab="x2",col=test$y+3,main="Test Data and Classifier Hyperplane")
z <- perceptron$z
abline((-z[3])/z[2],-z[1]/z[2])
#Second plot
iter <- nrow(perceptron$Z_history)
plot(training$S[,1],training$S[,2],xlab="x1",ylab="x2",col=training$y+3,main="Training Data and Trajectory of the Algorithm")
for (i in 1:(iter-1)){
z <- perceptron$Z_history[i,]
abline((-z[3])/z[2],-z[1]/z[2],lty=2)
}
z <- perceptron$Z_history[iter,]
abline((-z[3])/z[2],-z[1]/z[2],col="orange",lwd=2)
#Statistical Machine Learning (GR5241) HW2
#name: Chengcheng Yuan uni: cy2434
# Question 3. Perceptron
# 1.
classify <- function(S,z){
yhat <- as.vector(sign(S %*% z))
return(yhat)
}
#2.
perceptrain <- function(S,y){
n <- ncol(S)
z0 <- rep(1/2,n)
max.iter <- 200
step.size <- 1/max.iter
zmat <- matrix(0, nrow = max.iter, ncol = n)
zmat[1,] <- z0
for (k in 2:max.iter){
index <- as.numeric(classify(S,zmat[k-1, ])!=y)
cp <- sum(index*abs(classify(S,zmat[k-1, ])))
if (cp == 0) {
k <- k-1; break
}
a <- index*(-y)
cpgrad <- colSums(a*S)
zmat[k, ] <- zmat[k-1, ] - step.size * cpgrad
}
zmat <- zmat[1:k, ] # Trim
return(list(z = zmat[k, ], Z_history = zmat))
}
# check the perceptron function
fake <- fakedata(c(1,2,-3),100)
perceptron <- perceptrain(fake$S,fake$y)
perceptron$z
yhat <- classify(fake$S,perceptron$z)
sum(yhat==fake$y) #number of data that is classified correctly from the 100 data sample
#We can see from the result that the training data has all been classified correctly using my Perceptron training algorithm.
#3.
z_original <- sample(1:10,3) #generate a new three-dimensional random vector z
z_original
training <- fakedata(z_original,100)
perceptron <- perceptrain(training$S,training$y)
perceptron$z
test <- fakedata(z_original,100)
yhat <- classify(test$S,perceptron$z)
sum(yhat==test$y)/100
# The precision rate is usually above 95%, my classifier performs pretty well.
#4.
#First plot
plot(test$S[,1],test$S[,2],xlab="x1",ylab="x2",col=test$y+3,main="Test Data and Classifier Hyperplane")
z <- perceptron$z
abline((-z[3])/z[2],-z[1]/z[2])
#Second plot
iter <- nrow(perceptron$Z_history)
plot(training$S[,1],training$S[,2],xlab="x1",ylab="x2",col=training$y+3,main="Training Data and Trajectory of the Algorithm")
for (i in 1:(iter-1)){
z <- perceptron$Z_history[i,]
abline((-z[3])/z[2],-z[1]/z[2],lty=2)
}
z <- perceptron$Z_history[iter,]
abline((-z[3])/z[2],-z[1]/z[2],col="orange",lwd=2)
#Statistical Machine Learning (GR5241) HW2
#name: Chengcheng Yuan uni: cy2434
# Question 3. Perceptron
# 1.
classify <- function(S,z){
yhat <- as.vector(sign(S %*% z))
return(yhat)
}
#2.
perceptrain <- function(S,y){
n <- ncol(S)
z0 <- rep(1/2,n)
max.iter <- 200
step.size <- 1/max.iter
zmat <- matrix(0, nrow = max.iter, ncol = n)
zmat[1,] <- z0
for (k in 2:max.iter){
index <- as.numeric(classify(S,zmat[k-1, ])!=y)
cp <- sum(index*abs(classify(S,zmat[k-1, ])))
if (cp == 0) {
k <- k-1; break
}
a <- index*(-y)
cpgrad <- colSums(a*S)
zmat[k, ] <- zmat[k-1, ] - step.size * cpgrad
}
zmat <- zmat[1:k, ] # Trim
return(list(z = zmat[k, ], Z_history = zmat))
}
# check the perceptron function
fake <- fakedata(c(1,2,-3),100)
perceptron <- perceptrain(fake$S,fake$y)
perceptron$z
yhat <- classify(fake$S,perceptron$z)
sum(yhat==fake$y) #number of data that is classified correctly from the 100 data sample
#We can see from the result that the training data has all been classified correctly using my Perceptron training algorithm.
#3.
z_original <- sample(1:10,3) #generate a new three-dimensional random vector z
z_original
training <- fakedata(z_original,100)
perceptron <- perceptrain(training$S,training$y)
perceptron$z
test <- fakedata(z_original,100)
yhat <- classify(test$S,perceptron$z)
sum(yhat==test$y)/100
# The precision rate is usually above 95%, my classifier performs pretty well.
#4.
#First plot
plot(test$S[,1],test$S[,2],xlab="x1",ylab="x2",col=test$y+3,main="Test Data and Classifier Hyperplane")
z <- perceptron$z
abline((-z[3])/z[2],-z[1]/z[2])
#Second plot
iter <- nrow(perceptron$Z_history)
plot(training$S[,1],training$S[,2],xlab="x1",ylab="x2",col=training$y+3,main="Training Data and Trajectory of the Algorithm")
for (i in 1:(iter-1)){
z <- perceptron$Z_history[i,]
abline((-z[3])/z[2],-z[1]/z[2],lty=2)
}
z <- perceptron$Z_history[iter,]
abline((-z[3])/z[2],-z[1]/z[2],col="orange",lwd=2)
shiny::runApp('Documents/Michelle/Columbia stat/ads/project 2/Spr2017-proj2-grp6/doc/chengcheng')
load("readmission_map.RData")
setwd("~/Documents/Michelle/Columbia stat/ads/project 2/Spr2017-proj2-grp6/doc/chengcheng")
load("readmission_map.RData")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
