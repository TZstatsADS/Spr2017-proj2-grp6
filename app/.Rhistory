inner_join(AFINN, by = c(word2 = "word")) %>%
count(word1, word2, score, sort = TRUE) %>%
ungroup()
negation_words
negated_words %>%
mutate(contribution = n * score) %>%
mutate(word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
group_by(word1) %>%
top_n(5, abs(contribution)) %>%
ggplot(aes(word2, contribution, fill = n * score > 0)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ word1, scales = "free") +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
xlab("Words preceded by negation term") +
ylab("Sentiment score * # of occurrences") +
coord_flip()
require('maps')
require('countrycode')
require('rworldmap')
countries = tolower(unique(world.cities$country.etc)) # get country names
combined_speeches = PlainTextDocument(speech.corpus,language = "eng")
regions = c("europe","asia","africa","north america","south america","middle east","central america")
tf.countries = termFreq(combined_speeches,control = list(dictionary = countries))
tf.regions = termFreq(combined_speeches,control = list(dictionary = regions))
countries_mentioned <- names(as.list(tf.countries[tf.countries!=0]))
theCountries <- countrycode(countries_mentioned,'country.name','iso3c')
# These are the ISO3 names of the countries you'd like to plot in red
menDF <- data.frame(country = theCountries,
mentioned = TRUE)
menMap <- joinCountryData2Map(menDF, joinCode = "ISO3",
nameJoinColumn = "country")
# This will join your menDF data.frame to the country map data
mapCountryData(menMap, nameColumnToPlot="mentioned", catMethod = "categorical",
missingCountryCol = gray(.8), mapTitle = "Foreign Countries Mentioned in Inaugural Addresses", addLegend = FALSE)
# And this will plot it, with the trick that the color palette's first
# color is red
View(tb_words_nostop)
stop_words <- data("stop_words")
tb_words <- tb_speeches %>%
group_by(author) %>%
unnest_tokens(word, text, token = "words")
data(stop_words)
tb_words_nostop <- anti_join(tb_words,stop_words,by=c("word"="word"))
wordcloud <- tb_words_nostop %>%
group_by(word) %>%
count(word,sort=TRUE)
wordcloud(wordcloud$word,wordcloud$n,max.words=25)
dtm <- DocumentTermMatrix(corpus, control = list(removePunctuation = TRUE, stopwords = TRUE))
lda <- LDA(dtm,method="Gibbs",control = list(alpha=0.1),k=5) # running only a 5 topic model
topics <- topics(lda,5)
topics.hash
tb_words_nostop %>%
group_by(word) %>%
count(word, sort=TRUE)
tb_words_nostop %>%
count(word, sort = TRUE) %>%
filter(n > 150) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(y = "Mean Sentiment Over Sentences in Inaugural Address",
x = NULL) +
coord_flip()
tb_words_nostop %>%
count(word, sort = TRUE) %>%
filter(n > 150) %>%
mutate(word = reorder(word, n)) %>%
ungroup()
ggplot(aes(word, n)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(y = "Mean Sentiment Over Sentences in Inaugural Address",
x = NULL) +
coord_flip()
tb_words_nostop %>%
count(word, sort = TRUE) %>%
filter(n > 150) %>%
mutate(word = reorder(word, n)) %>%
ungroup() %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(y = "Mean Sentiment Over Sentences in Inaugural Address",
x = NULL) +
coord_flip()
tidy_comparison <- bind_rows(trump <- filter(tb_words_nostop, author=="Donald J. Trump"),
clinton <- filter(tb_words_nostop, author=="William J. Clinton"))
stop_words <- data("stop_words")
tb_words <- tb_speeches %>%
unnest_tokens(word, text, token = "words")
data(stop_words)
tb_words_nostop <- anti_join(tb_words,stop_words,by=c("word"="word"))
wordcloud <- tb_words_nostop %>%
group_by(word) %>%
count(word,sort=TRUE)
wordcloud(wordcloud$word,wordcloud$n,max.words=25)
tb_words_nostop %>%
group_by(word) %>%
count(word, sort=TRUE)
tb_words_nostop %>%
count(word, sort = TRUE) %>%
filter(n > 150) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(y = "Mean Sentiment Over Sentences in Inaugural Address",
x = NULL) +
coord_flip()
tb_words_nostop %>%
count(word, sort = TRUE) %>%
filter(n > 150) %>%
mutate(word = reorder(word, n)) %>%
group_by(author) %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(y = "Mean Sentiment Over Sentences in Inaugural Address",
x = NULL) +
coord_flip()
tb_words_nostop %>%
count(word, sort = TRUE) %>%
filter(n > 150) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(y = "Mean Sentiment Over Sentences in Inaugural Address",
x = NULL) +
coord_flip()
tb_words_nostop %>%
group_by(author) %>%
group_by(word) %>%
count(word, sort=TRUE)
tb_words_nostop %>%
group_by(author,word) %>%
count(word, sort=TRUE)
tb_words_nostop %>%
group_by(party,word) %>%
count(word, sort=TRUE)
tidy_comparison <- bind_rows(trump <- filter(tb_words_nostop, author=="Donald J. Trump"),
clinton <- filter(tb_words_nostop, author=="William J. Clinton"))
obama_percent <- tb_words_nostop %>%
mutate(word = str_extract(words, "[a-z']+")) %>%
count(word) %>%
transmute(word, obama = n / sum(n))
tidy_comparison <- bind_rows(trump <- filter(tb_words_nostop, author=="Donald J. Trump"),
clinton <- filter(tb_words_nostop, author=="William J. Clinton"))
obama_percent <- tb_words_nostop %>%
mutate(word = str_extract(words, "[a-z']+")) %>%
count(word) %>%
transmute(word, obama = n / sum(n))
tidy_comparison <- bind_rows(trump <- filter(clean_word_tb, author=="Donald J. Trump"),
clinton <- filter(clean_word_tb, author=="William J. Clinton"))
obama_percent <- clean_word_tb %>%
mutate(word = str_extract(words, "[a-z']+")) %>%
count(word) %>%
transmute(word, obama = n / sum(n))
frequency <- tidy_comparison %>%
mutate(word = str_extract(words, "[a-z']+")) %>%
count(author, word) %>%
mutate(other = n / sum(n)) %>%
left_join(obama_percent) %>%
ungroup()
library(scales)
ggplot(frequency, aes(x = other, y = obama, color = abs(obama - other))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Barack Obama", x = NULL)
cor.test(data = frequency[frequency$author == "Donald J. Trump",],
~ other + obama)
cor.test(data = frequency[frequency$author == "William J. Clinton",],
~ other + obama)
clean_tb<-tb
clean_tb$text <-removePunctuation(tb$text,preserve_intra_word_dashes = TRUE)
clean_tb$text <-removeNumbers(tb$text)
clean_tb$text <-removeWords(tb$text,stopwords("english"))
word_bigram <- clean_tb %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
word_bigram
dtm <- DocumentTermMatrix(corpus, control = list(removePunctuation = TRUE, stopwords = TRUE))
lda <- LDA(dtm,method="Gibbs",control = list(alpha=0.1),k=5) # running only a 5 topic model
topics <- topics(lda,5)
library(swirl)
swirl()
x<-"Hello World!"
x
paste("Square","Circle","Triangle")
paste("Square","Circle","Triangle",sep = "+")
paste0("Square","Circle","Triangle")
shapes <- c("Square","Cirlce","Triangle")
shapes <- c("Square","Circle","Triangle")
paste("My favorite shape is a",shapes)
paste(shapes,collapse = " ")
nchar("Count Me!")
cases <- c("CAPS","low","Title")
toupper(cases)
tolower(cases)
regular_expression <- "a"
string_to_search <- "Maryland"
grepl(regular_expression,string_to_search)
grepl("ryla", "Maryland")
grepl("Marly", "Maryland")
head(state.name)
grepl(".", "Maryland")
grepl(".",
| "")
grepl(".", "")
grepl("a.b", c("aaa",
| "aab", "abb", "acadb"))
grepl("a.b", c("aaa", "aab", "abb", "acadb"))
grepl("a+","Maryland")
grepl("x*","Maryland")
grepl("ss{1}","Mississippi")
grepl("s{2}", "Mississippi")
grepl("i{2,3}","Mississippi")
grepl("(iss){2}","Mississippi")
grepl("\\d","0123456789")
grepl("\\D","0123456789")
grepl("[aeiou]","rhythms")
grepl("[.]","http://www.jhsph.edu/")
grepl("\\.","http://www.jhsph.edu/")
grepl("^a", c("bab", "aab"))
grepl("$b", c("bab", "aab"))
grepl("$b", c("bab", "aab"))
info()
grepl("\$b", c("bab", "aab"))
grepl(""$b", c("bab", "aab"))
grepl("^b", c("bab", "aab"))
grepl("$b", c("bab", "aab"))
grepl("b$", c("bab", "aab"))
grepl("a|b", c("abc", "bcd", "cde"))
start_end_vowel <- "^[AEIOU]{1}.+[aeiou]{1}$"
grepl(start_end_vowel,state.name)
vowel_state_lgl <- grepl(start_end_vowel, state.name)
state.name[vowel_state_lgl]
grepl("[Ii]",c("Hawaii","Illinois","Kentucky"))
grep("[Ii]",c("Hawaii","Illinois","Kentucky"))
sub("[Ii]",c("Hawaii","Illinois","Kentucky"))
sub("[Ii]",1,c("Hawaii","Illinois","Kentucky"))
sub("[Ii]","1",c("Hawaii","Illinois","Kentucky"))
gsub("[Ii]","1",c("Hawaii","Illinois","Kentucky"))
two_s<- state.name[grep("ss",state.name)]
two_s
strsplit(two_s,"ss")
str_extract("Camaro Z28", "[0-9]+")
str_order(c("p", "e", "n", "g"))
str_pad("Thai", width = 8, side = "left", pad = "-")
str_to_title(c("CAPS", "low", "Title"))
str_trim(" trim me ")
word("See Spot run.", 2)
library(pryr)
mem_used()
111 MB
object_size(worldcup)
object_size(ls())
sapply(ls(), function(x) object_size(get(x))) %>% sort %>% tail(5)
str(.Machine)
library(dplyr)
library(stringr)
library(tidyr)
library(readr)
hospital_charge_data <- read_csv("~/Google Drive/Applied Data Science Projects/Proj2/Spr2017-proj2-grp6/data/hospital_charge_data.csv", col_types = cols(`Average Covered Charges` = col_number(),`Average Medicare Payments` = col_number(),`Average Total Payments` = col_number()))
cnames <- colnames(hospital_charge_data)
new_cnames <- tolower(gsub("[ ]","_",cnames,perl=FALSE))
new_cnames -> colnames(hospital_charge_data)
drg <- as.numeric(str_extract(hospital_charge_data$drg_definition,"[0-9]+"))
mdf <- hospital_charge_data %>% mutate(drg_code = drg)
hospital_charges <- mdf %>%
select(drg_definition,total_discharges,provider_id,average_covered_charges) %>%
mutate(charge_total = total_discharges*average_covered_charges) %>%
group_by(provider_id) %>%
summarise(provider_gross_charges = sum(charge_total))
# can chart a distribution of revenues by hospital?
top_hospitals_billing <- hospital_charges %>%
top_n(20)
top_billing_procedures <- mdf %>%
select(drg_definition,drg_code,total_discharges,provider_id,average_covered_charges) %>%
mutate(charge_total = total_discharges*average_covered_charges) %>%
group_by(drg_code) %>%
summarise(drg_gross_charges = sum(charge_total)) %>%
top_n(20)
hos<-unique(top_hospitals_billing$provider_id)
proc<-unique(top_billing_procedures$drg_code)
hmdata <- mdf %>%
filter(drg_code %in% proc & provider_id %in% hos) %>%
select(drg_definition,provider_name,total_discharges,average_covered_charges) %>%
mutate(charge_total = total_discharges*average_covered_charges)
library(ggplot2)
ggplot(data = hmdata, aes(x = provider_name, y = drg_definition)) +
geom_tile(aes(fill = charge_total)) +
scale_fill_gradient2(low="darkblue", high="darkgreen", guide="colorbar") +
theme(axis.text.x = element_text(angle = -90, hjust = 0))
heart_related <- mdf %>%
select(drg_definition, drg_code, total_discharges, provider_zip_code) %>%
mutate(heart_related = str_detect(drg_definition,"CARDIAC")) %>%
filter(heart_related %in% TRUE) %>%
group_by(provider_zip_code) %>%
summarise(cardiac_discharges = sum(total_discharges))
all_discharges <- mdf %>%
select(total_discharges,provider_zip_code) %>%
group_by(provider_zip_code) %>%
summarise(all_discharges = sum(total_discharges))
ij<-inner_join(all_discharges,heart_related)
ij<-ij %>% mutate(zip = as.character(provider_zip_code),
pct_cardiac = cardiac_discharges/all_discharges)
data("zip.regions")
cz<-zip.regions %>%
select(region,county.name,state.name)
data("zip.regions")
library(dplyr)
library(stringr)
library(tidyr)
library(readr)
library(choroplethr)
library(choroplethrMaps)
library(choroplethrZip)
library(dplyr)
library(stringr)
library(tidyr)
library(readr)
library(choroplethr)
library(choroplethrMaps)
library(choroplethrZip)
data("zip.regions")
cz<-zip.regions %>%
select(region,county.name,state.name)
ij<-ij %>%
filter(zip %in% cz$region)
cj<-semi_join(ij,cz,by=c("zip" = "region"))
zip_data <- cj %>% select(zip,pct_cardiac)
colnames(zip_data)<-c("region","value")
zip_choropleth(zip_data, title = "Heart Related Discharges as Pct of Total Discharges (By Zip Code)", state_zoom = "new york")
View(zip.regions)
View(mdf)
unique(mdf$drg_definition)
View(mdf)
library(readr)
Hospital_Value_Based_Purchasing_HVBP_Outcome_Scores <- read_csv("~/Downloads/Hospital_Value-Based_Purchasing__HVBP____Outcome_Scores.csv")
View(Hospital_Value_Based_Purchasing_HVBP_Outcome_Scores)
View(Hospital_Value_Based_Purchasing_HVBP_Outcome_Scores)
library(readr)
Readmissions_and_Deaths_Hospital <- read_csv("~/Google Drive/Applied Data Science Projects/Proj2/Spr2017-proj2-grp6/data/Readmissions_and_Deaths_-_Hospital.csv")
View(Readmissions_and_Deaths_Hospital)
"010023" %in% mdf$provider_id
"10023" %in% mdf$provider_id
mdf %>% filter("10023" %in% provider_id) %>% select(provider_name)
unique(Readmissions_and_Deaths_Hospital$`Measure Name`)
data("zip.regions")
library(choroplethrMaps)
data("zip.regions")
library(choroplethrZip)
data("zip.regions")
install.packages("fastAdaboost")
install.packages("fastAdaboost")
setwd("~/Google Drive/Applied Data Science Projects/Proj2/Spr2017-proj2-grp6/app")
shiny::runApp()
install.packages("shinythemes")
runApp()
runApp()
runApp('new_app/server.R')
runApp('new_app/server.R')
runApp('new_app/server.R')
runApp('new_app/server.R')
?navbarPage
runApp()
runApp()
runApp()
runApp()
fluidRow()
?tabPanel
?navbarPage
runApp()
?navbarMenu
runApp()
?navbarPage
runApp()
runApp()
?renderDataTable
?dataTableOutput
runApp()
library(DT)
runApp()
runApp()
?dataTableOutput
runApp()
?fluidRow
?navbarPage
runApp()
?fluidRow
runApp()
?selectInput
?selectizeInput
runApp('new_app/server.R')
source('~/Google Drive/Applied Data Science Projects/Proj2/Spr2017-proj2-grp6/app/lib/cost_filtering_utils.R')
runApp()
runApp()
?plotlyOutput
runApp()
runApp()
hcd$state_id[1]
hcd$state_name[1]
runApp()
runApp()
runApp()
df <- hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise, rel_cost = average_covered_charges/median_income) %>%
select(state_name, rel_cost)
df
df
hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise, rel_cost = average_covered_charges/median_income) %>%
select(state_name, rel_cost)
drg<-"039"
df<- hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise, rel_cost = average_covered_charges/median_income) %>%
select(state_name, rel_cost)
View(hcd)
df <- hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise,
avg_chg = (total_discharges*average_covered_charges)/sum(total_discharges),
rel_chg = avg_chg/median_income)
df <- hcd %>%
filter(drg_code %in% drg)
df <- hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise,
avg_chg = (total_discharges*average_covered_charges)/sum(total_discharges)) %>%
mutate(rel_cost = avg_chg/median_income) %>%
select(state_name, rel_cost)
df <- hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise,
avg_covered = sum(total_discharges*average_covered_charges)/sum(total_discharges))
View(df)
df <- hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise,
avg_covered = sum(total_discharges*average_covered_charges)/sum(total_discharges), median_income)
df <- hcd %>%
filter(drg_code %in% drg) %>%
ddply(.(state_name), summarise,
avg_covered = sum(total_discharges*average_covered_charges)/sum(total_discharges), income = mean(median_income))
runApp()
runApp()
runApp()
runApp()
includeMarkdown("www/text/test.txt")
includeText("www/text/test.txt")
runApp()
runApp()
runApp()
runApp()
?renderPlotly
?plot_ly
runApp()
runApp()
round(123.4)
runApp()
rnorm(50)
typeof(rnorm(50))
install.packages("d3heatmap")
url <- "http://datasets.flowingdata.com/ppg2008.csv"
nba_players <- read.csv(url, row.names = 1)
d3heatmap(nba_players, scale = "column")
library(d3heatmap)
url <- "http://datasets.flowingdata.com/ppg2008.csv"
nba_players <- read.csv(url, row.names = 1)
d3heatmap(nba_players, scale = "column")
View(nba_players)
structure(nba_players)
runApp()
prov <- hcd %>%
ddply(.(provider_name), summarise,
total_charges = sum(total_discharges*average_covered_charges)) %>%
top_n(20)
proc <- hcd %>%
ddply(.(drg_definition), summarise,
total_charges = sum(total_discharges*average_covered_charges)) %>%
top_n(20)
desc <- hcd %>%
select(provider_name, provider_id, drg_definition, drg_code) %>%
distinct()
hm <- left_join(prov,proc,desc)
prov <- hcd %>%
ddply(.(provider_name), summarise,
total_charges = sum(total_discharges*average_covered_charges)) %>%
top_n(20)
prov
proc <- hcd %>%
ddply(.(drg_definition), summarise,
total_charges = sum(total_discharges*average_covered_charges)) %>%
top_n(20)
proc
desc <- hcd %>%
select(provider_name, provider_id, drg_definition, drg_code) %>%
distinct()
desc
proc <- hcd %>%
ddply(.(drg_definition), summarise,
total_charges = sum(total_discharges*average_covered_charges)) %>%
top_n(20)
prov <- hcd %>%
filter(drg_definition %in% proc$drg_definition) %>%
ddply(.(provider_name,drg_definition), summarise,
total_charges = sum(total_discharges*average_covered_charges)) %>%
top_n(20)
